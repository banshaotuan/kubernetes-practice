### kubernetes 背景



### kubernetes  入门知识



### kubernetes-pod

为了确保容器处于健康的运行状态，kubernets提供了两种探针来探测容器的状态




###  livenessProbe

探测应用是否处于健康状态，如果不健康则删除重建修改容器

```
apiVersion: v1
kind: Pod
metadata:
  labels:
    test: liveness
  name: liveness
spec:
  containers:
  - name: liveness
    image: busybox
    args:
    - /bin/sh
    - -c
    - touch /tmp/healthy; sleep 30; rm -rf /tmp/healthy; sleep 600
    livenessProbe:
      exec:
        command:
        - cat
        - /tmp/healthy
      initialDelaySeconds 5
      periodSeconds: 5
```

`initalDelaySeconds:容器启动多长时间之后开始进行健康`

`periodSeconds: 多长时间执行一次健康检查`

创建了此Pod之后，kubectl get pod liveness 查看到该pod的状态是running，说明容器启动成功，且通过了健康检查，describe查看pod的event事件也正常，过一段时间过后，查看Event显示错误信息，此时get pod查看状态还是Running，但是RESTARTS数量已经不为0，此时表明pod异常，kubernetes已经重启此pod（说是重启，其实是创建新的pod）

这个功能就是kubernetes的Pod恢复机制，也叫做restartPolicy，它是Pod的spec部分的一个标准字段(pod.spec.restartPolicy),默认是是Alway，即:任何时候容器发生了异常导致健康检查失败，pod一定会被重新创建。

此外需要注意的事情是，Pod的恢复过程，永远都是发生在当前节点上，而不会跑到另外的node节点上，一旦一个Pod与一个节点（Node）绑定，除非这个绑定的节点出现了（nodeSelector或者nodeName）变动，否则这个Pod永远不会转移到其他的节点上，这也就意味着，如果这个Node节点宕机了，这个Pod也不会迁移到其他正常的Node节点上

如果你想让Pod出现在其他的节点上，就必须使用Deployment这样的控制器来管理Pod


###  ReadinessProbe
探测应用是否启动完成并处于正常服务状态，如果不正常则更新容器的状态





#### spec.resoures

可以使用resource选项来限制容器需要的最小资源
limits用于限制运行时容器占用的资源，用来限制容器的最大CPU，内存的使用率
当容器申请内存超过limit时会被终止，并根据重启策略进行重启



#### spec.containers.imagePullPolicy
可选项: Never Alway  IfNotPresent

此字段定义了镜像拉取的策略，imagePullPolicy默认的策略是IfNotPresent（视版本而定）

#### spec.hostAliases
此选项的作用是添加host主机解析
语法如下

```
spec:
  hostAliases:
  - ip: '127.0.0.1'
    hostnames:
    - 'localhost'
    - 'local'
  - ip '192.168.10.1'
    hostnames:
    - 'kube-node1'
    - 'kube-node2'

```

```

appVersion: v1
kind: Pod
metadata:
  name：busybox
spec:
  containers:
  - name: busybox
    image: busybox:latest
    ports:
    - containerPort: 80
    env：
      name: FLAVOR
      value: test
    
    volumeMount:
    - name: busybox
      mountPth: "/usr/share/nginx/html"
 
 ```
 
 
### spec.containers.lifecycle

~~~ 此字段可以在pod启动后和容器被杀死之前设置相关的动作
postStart：pod启动之后执行，立刻会执行postStatr里面设定的动作，有一点需要注意，虽然是在docker容器的ENTRYPOINT执行之后在执行的，但是他并不严格保证顺序，也就是说，在postStart启动的时候，有可能EENTRYPOINT还没有返回结果

preStop：与postStart不一样的是，当容器收到呗kill掉的指令之后，preStop会阻塞此kill命令的执行，知道preStop执行成功，容器才会被kill掉，
~~~

> 案例如下：

```
apiVersion: v1
kind: Pod
metadata:
  name: lifecycle-demo
spec:
  containers:
  - name: lifecycle-demo
    image: nginx:latest
    lifecycle:
      postStart:
        exec:
          command: ["/bin/sh","-c","echo postStart > /usr/share/nginx/html/lifecycle_start.txt"]
      preStop:
        exec:
          command: ["/bin/sh","-c","echo portStop > /usr/share/nginx/html/lifecycle_stop.txt"]
    volumeMounts:
    - mountPath: "/usr/share/nginx/html"
      name: lifecycle-vol
  volumes:
    - name: lifecycle-vol
      hostPath:
        path: "/data/lifecycle-demo"
```

> 启动pod之后

```
[root@kube-node1 banshaotaun]# kubectl create -f lifecycle.yaml 
pod "lifecycle-demo" created
[root@kube-node1 banshaotaun]# kubectl get pod  -o wide|grep life
lifecycle-demo                           1/1       Running            0          9s        172.30.95.9    kube-node1
[root@kube-node1 banshaotaun]# 
```


> 查看postStart生成的文件

```
[root@kube-node1 lifecycle-demo]# pwd
/data/lifecycle-demo
[root@kube-node1 lifecycle-demo]# ls
lifecycle_start.txt
[root@kube-node1 lifecycle-demo]# cat lifecycle_start.txt 
postStart
[root@kube-node1 lifecycle-demo]# 

```

> kill掉Pod
```
[root@kube-node1 banshaotaun]# kubectl delete pod lifecycle-demo
pod "lifecycle-demo" deleted
[root@kube-node1 banshaotaun]# 
```


> 查看preStop生成的文件
```
[root@kube-node1 lifecycle-demo]# pwd
/data/lifecycle-demo
[root@kube-node1 lifecycle-demo]# ls
lifecycle_start.txt  lifecycle_stop.txt
[root@kube-node1 lifecycle-demo]# cat lifecycle_stop.txt 
portStop
[root@kube-node1 lifecycle-demo]# 


```

### kubernetes-Deployment




### kubernetes-PodPreset
